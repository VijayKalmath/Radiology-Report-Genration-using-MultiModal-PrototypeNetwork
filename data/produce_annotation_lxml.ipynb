{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7864a33",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Notebook used to produce 'annotations.json' for the iu_xray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc39368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e32e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join('.','iu_xray', 'images')\n",
    "report_path = os.path.join('.','iu_xray', 'reports')\n",
    "imgs = os.listdir(img_path)\n",
    "reports = os.listdir(report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e676c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CXR1000_IM-0003-1001.png',\n",
       " 'CXR1000_IM-0003-2001.png',\n",
       " 'CXR1000_IM-0003-3001.png',\n",
       " 'CXR1001_IM-0004-1001.png',\n",
       " 'CXR1001_IM-0004-1002.png',\n",
       " 'CXR1002_IM-0004-1001.png',\n",
       " 'CXR1002_IM-0004-2001.png',\n",
       " 'CXR1003_IM-0005-2002.png',\n",
       " 'CXR1004_IM-0005-1001.png',\n",
       " 'CXR1004_IM-0005-2001.png']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe names of images\n",
    "imgs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e39512de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.xml',\n",
       " '10.xml',\n",
       " '100.xml',\n",
       " '1000.xml',\n",
       " '1001.xml',\n",
       " '1002.xml',\n",
       " '1003.xml',\n",
       " '1004.xml',\n",
       " '1005.xml',\n",
       " '1006.xml']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe names of xml files\n",
    "reports[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e455c116",
   "metadata": {},
   "source": [
    "Testing on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9323a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta\n",
      "uId\n",
      "pmcId\n",
      "docSource\n",
      "IUXRId\n",
      "licenseType\n",
      "licenseURL\n",
      "ccLicense\n",
      "articleURL\n",
      "articleDate\n",
      "articleType\n",
      "publisher\n",
      "title\n",
      "note\n",
      "specialty\n",
      "subset\n",
      "MedlineCitation\n",
      "MeSH\n",
      "parentImage\n",
      "parentImage\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The cardiac silhouette and mediastinum size are within normal limits. There is no pulmonary edema. There is no focal consolidation. There are no XXXX of a pleural effusion. There is no evidence of pneumothorax.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(report_path, reports[0]), 'r') as f:\n",
    "    data = f.read()\n",
    "tree = ET.parse(os.path.join(report_path, reports[0]))\n",
    "root = tree.getroot()\n",
    "for e in root:\n",
    "    print(e.tag)\n",
    "root.findall(\".//AbstractText[@Label='FINDINGS']\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8a89cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 0.0044307708740234375 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image_path': ['CXR1_1_IM-0001-3001.png', 'CXR1_1_IM-0001-4001.png'],\n",
       " 'id': 'CXR1_1_IM-0001-3001',\n",
       " 'report': 'The cardiac silhouette and mediastinum size are within normal limits. There is no pulmonary edema. There is no focal consolidation. There are no XXXX of a pleural effusion. There is no evidence of pneumothorax.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "img_dict = {'image_path': []}\n",
    "tree = ET.parse(os.path.join(report_path, reports[0]))\n",
    "root = tree.getroot()\n",
    "pi = root.findall('parentImage')\n",
    "if len(pi)<2:\n",
    "    print('cannot find enough imgs')\n",
    "else:\n",
    "    img_dict['id'] = pi[0].attrib['id']\n",
    "    for img_id in pi:\n",
    "        img_dict['image_path'].append(img_id.attrib['id']+'.png')\n",
    "    img_dict['report'] = root.findall(\".//AbstractText[@Label='FINDINGS']\")[0].text\n",
    "print('This took {} seconds'.format(time.time()-start))\n",
    "img_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d2ee9",
   "metadata": {},
   "source": [
    "### Reading all reports and storing them in train, val, and test randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2178c5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.69963812828064 seconds to process 3955 reports\n",
      "Total reports in dataset: 2955\n",
      "Reports not included due to lack of images or lack of Report Text: 1000\n"
     ]
    }
   ],
   "source": [
    "img_suffix = imgs[0][imgs[0].find('.'):]\n",
    "num_reports = 0\n",
    "report_list = []\n",
    "start = time.time()\n",
    "for report in reports:\n",
    "    \n",
    "    img_dict = {'image_path': []}\n",
    "    tree = ET.parse(os.path.join(report_path, report))\n",
    "    root = tree.getroot()\n",
    "    pi = root.findall('parentImage')\n",
    "    if len(pi)<2:\n",
    "        pass\n",
    "        #print('not enough images! skipping {}'.format(report))\n",
    "    else:\n",
    "        img_dict['id'] = pi[0].attrib['id']\n",
    "        for img_id in pi:\n",
    "            img_dict['image_path'].append(img_id.attrib['id']+'.png')\n",
    "        img_dict['report'] = root.findall(\".//AbstractText[@Label='FINDINGS']\")[0].text\n",
    "        if img_dict['report'] == None:\n",
    "            continue\n",
    "        num_reports += 1\n",
    "        report_list.append(img_dict)\n",
    "        \n",
    "        \n",
    "print('{} seconds to process {} reports'.format(time.time()-start, len(reports)))\n",
    "print('Total reports in dataset: {}'.format(num_reports))\n",
    "print('Reports not included due to lack of images or lack of Report Text: {}'.format(len(reports)-num_reports))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a1e7731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2955\n"
     ]
    }
   ],
   "source": [
    "# double check number of reports\n",
    "print(len(report_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbaea80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev size: 2364, test size: 591\n",
      "Train size: 1773, val size: 591\n"
     ]
    }
   ],
   "source": [
    "# use sklearn to split into train, validation, and test sets\n",
    "dev, test, _, _ = train_test_split(report_list, range(len(report_list)), test_size=0.2, random_state=42)\n",
    "print('Dev size: {}, test size: {}'.format(len(dev),len(test)))\n",
    "train, val, _, _ = train_test_split(dev, range(len(dev)), test_size=0.25, random_state=42)\n",
    "print('Train size: {}, val size: {}'.format(len(train),len(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a5a19be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to dictionary with train, val, and test partitions\n",
    "annotations = {'train': train, 'val': val, 'test': test}\n",
    "\n",
    "# convert dictionary to json object\n",
    "annotations_json = json.dumps(annotations)\n",
    "\n",
    "# write to json file\n",
    "with open(\"annotation.json\", \"w\") as f:\n",
    "    f.write(annotations_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('XProNet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4f9dab55187a644dc9216a574b8bf5a5414f59e9d3120c4b3155988d88cf5e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

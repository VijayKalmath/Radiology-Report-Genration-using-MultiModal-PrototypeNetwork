{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08db36d4",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Notebook used to produce 'annotations.json' for the iu_xray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a4b08c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "90236820",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join('.','iu_xray', 'images')\n",
    "report_path = os.path.join('.','iu_xray', 'reports')\n",
    "imgs = os.listdir(img_path)\n",
    "reports = os.listdir(report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8e306250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CXR163_IM-0410-12012.png',\n",
       " 'CXR2595_IM-1086-2001.png',\n",
       " 'CXR1465_IM-0302-2001.png',\n",
       " 'CXR2835_IM-1251-1001.png',\n",
       " 'CXR855_IM-2376-1001.png',\n",
       " 'CXR444_IM-2079-2001.png',\n",
       " 'CXR3059_IM-1425-2001.png',\n",
       " 'CXR2504_IM-1029-2001.png',\n",
       " 'CXR2395_IM-0944-1001.png',\n",
       " 'CXR776_IM-2319-2001.png']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe names of images\n",
    "imgs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "73071927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['162.xml',\n",
       " '1390.xml',\n",
       " '604.xml',\n",
       " '2699.xml',\n",
       " '2841.xml',\n",
       " '3587.xml',\n",
       " '2855.xml',\n",
       " '3593.xml',\n",
       " '88.xml',\n",
       " '610.xml']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe names of xml files\n",
    "reports[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36799478",
   "metadata": {},
   "source": [
    "Testing on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a22614fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(report_path, reports[0]), 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "83c18703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 0.008481979370117188 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image_path': ['CXR162_IM-0401-1001.png', 'CXR162_IM-0401-2001.png'],\n",
       " 'id': 'CXR162_IM-0401-1001',\n",
       " 'report': 'Heart size normal. Lungs are clear. XXXX are normal. No pneumonia, effusions, edema, pneumothorax, adenopathy, nodules or masses.'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "img_dict = {'image_path': []}\n",
    "bs = BeautifulSoup(data, \"xml\")\n",
    "pi = bs.find_all('parentImage')\n",
    "if len(pi)<2:\n",
    "    print('not enough images! skipping',pi[0]['id'])\n",
    "else:\n",
    "    img_dict['id'] = pi[0]['id']\n",
    "    for img_id in pi:\n",
    "        img_dict['image_path'].append(img_id['id']+'.png')\n",
    "    img_dict['report'] = bs.find('AbstractText', {'Label': 'FINDINGS'}).text\n",
    "print('This took {} seconds'.format(time.time()-start))\n",
    "img_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "26bd005f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Heart size normal. Lungs are clear. XXXX are normal. No pneumonia, effusions, edema, pneumothorax, adenopathy, nodules or masses.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.find('AbstractText', {'Label': 'FINDINGS'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83360dce",
   "metadata": {},
   "source": [
    "### Reading all reports and storing them in train, val, and test randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ebecfe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.789681196212769 seconds to process 3955 reports\n",
      "Total reports in dataset: 3405\n",
      "Reports not included due to lack of images: 550\n"
     ]
    }
   ],
   "source": [
    "img_suffix = imgs[0][imgs[0].find('.'):]\n",
    "num_reports = 0\n",
    "report_list = []\n",
    "start = time.time()\n",
    "for report in reports:\n",
    "    with open(os.path.join(report_path, report), 'r') as f:\n",
    "        data = f.read()\n",
    "    img_dict = {'image_path': []}\n",
    "    bs = BeautifulSoup(data, \"xml\")\n",
    "    pi = bs.find_all('parentImage')\n",
    "    if len(pi)<2:\n",
    "        pass\n",
    "        #print('not enough images! skipping {}'.format(report))\n",
    "    else:\n",
    "        num_reports += 1\n",
    "        img_dict['id'] = pi[0]['id']\n",
    "        for img_id in pi:\n",
    "            img_dict['image_path'].append(img_id['id']+img_suffix)\n",
    "        img_dict['report'] = bs.find('AbstractText', {'Label': 'FINDINGS'}).text\n",
    "        report_list.append(img_dict)\n",
    "        \n",
    "        \n",
    "print('{} seconds to process {} reports'.format(time.time()-start, len(reports)))\n",
    "print('Total reports in dataset: {}'.format(num_reports))\n",
    "print('Reports not included due to lack of images: {}'.format(len(reports)-num_reports))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a36b134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3405\n"
     ]
    }
   ],
   "source": [
    "# double check number of reports\n",
    "print(len(report_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4d34f450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev size: 2724, test size: 681\n",
      "Train size: 2043, val size: 681\n"
     ]
    }
   ],
   "source": [
    "# use sklearn to split into train, validation, and test sets\n",
    "dev, test, _, _ = train_test_split(report_list, range(len(report_list)), test_size=0.2, random_state=42)\n",
    "print('Dev size: {}, test size: {}'.format(len(dev),len(test)))\n",
    "train, val, _, _ = train_test_split(dev, range(len(dev)), test_size=0.25, random_state=42)\n",
    "print('Train size: {}, val size: {}'.format(len(train),len(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a626fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to dictionary with train, val, and test partitions\n",
    "annotations = {'train': train, 'val': val, 'test': test}\n",
    "\n",
    "# convert dictionary to json object\n",
    "annotations_json = json.dumps(annotations)\n",
    "\n",
    "# write to json file\n",
    "with open(\"annotations.json\", \"w\") as f:\n",
    "    f.write(annotations_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7864a33",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Notebook used to produce 'annotations.json' for the iu_xray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc39368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1e32e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join('.','iu_xray', 'images')\n",
    "report_path = os.path.join('.','iu_xray', 'reports')\n",
    "imgs = os.listdir(img_path)\n",
    "reports = os.listdir(report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e676c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CXR163_IM-0410-12012.png',\n",
       " 'CXR2595_IM-1086-2001.png',\n",
       " 'CXR1465_IM-0302-2001.png',\n",
       " 'CXR2835_IM-1251-1001.png',\n",
       " 'CXR855_IM-2376-1001.png',\n",
       " 'CXR444_IM-2079-2001.png',\n",
       " 'CXR3059_IM-1425-2001.png',\n",
       " 'CXR2504_IM-1029-2001.png',\n",
       " 'CXR2395_IM-0944-1001.png',\n",
       " 'CXR776_IM-2319-2001.png']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe names of images\n",
    "imgs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e39512de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['162.xml',\n",
       " '1390.xml',\n",
       " '604.xml',\n",
       " '2699.xml',\n",
       " '2841.xml',\n",
       " '3587.xml',\n",
       " '2855.xml',\n",
       " '3593.xml',\n",
       " '88.xml',\n",
       " '610.xml']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe names of xml files\n",
    "reports[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e455c116",
   "metadata": {},
   "source": [
    "Testing on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9323a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta\n",
      "uId\n",
      "pmcId\n",
      "docSource\n",
      "IUXRId\n",
      "licenseType\n",
      "licenseURL\n",
      "ccLicense\n",
      "articleURL\n",
      "articleDate\n",
      "articleType\n",
      "publisher\n",
      "title\n",
      "note\n",
      "specialty\n",
      "subset\n",
      "MedlineCitation\n",
      "MeSH\n",
      "parentImage\n",
      "parentImage\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Heart size normal. Lungs are clear. XXXX are normal. No pneumonia, effusions, edema, pneumothorax, adenopathy, nodules or masses.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(report_path, reports[0]), 'r') as f:\n",
    "    data = f.read()\n",
    "tree = ET.parse(os.path.join(report_path, reports[0]))\n",
    "root = tree.getroot()\n",
    "for e in root:\n",
    "    print(e.tag)\n",
    "root.findall(\".//AbstractText[@Label='FINDINGS']\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a89cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 0.0010399818420410156 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image_path': ['CXR162_IM-0401-1001.png', 'CXR162_IM-0401-2001.png'],\n",
       " 'id': 'CXR162_IM-0401-1001',\n",
       " 'report': 'Heart size normal. Lungs are clear. XXXX are normal. No pneumonia, effusions, edema, pneumothorax, adenopathy, nodules or masses.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "img_dict = {'image_path': []}\n",
    "tree = ET.parse(os.path.join(report_path, reports[0]))\n",
    "root = tree.getroot()\n",
    "pi = root.findall('parentImage')\n",
    "if len(pi)<2:\n",
    "    print('cannot find enough imgs')\n",
    "else:\n",
    "    img_dict['id'] = pi[0].attrib['id']\n",
    "    for img_id in pi:\n",
    "        img_dict['image_path'].append(img_id.attrib['id']+'.png')\n",
    "    img_dict['report'] = root.findall(\".//AbstractText[@Label='FINDINGS']\")[0].text\n",
    "print('This took {} seconds'.format(time.time()-start))\n",
    "img_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d2ee9",
   "metadata": {},
   "source": [
    "### Reading all reports and storing them in train, val, and test randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2178c5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5169458389282227 seconds to process 3955 reports\n",
      "Total reports in dataset: 2955\n",
      "Reports not included due to lack of images or lack of Report Text: 1000\n"
     ]
    }
   ],
   "source": [
    "img_suffix = imgs[0][imgs[0].find('.'):]\n",
    "num_reports = 0\n",
    "report_list = []\n",
    "start = time.time()\n",
    "for report in reports:\n",
    "    \n",
    "    img_dict = {'image_path': []}\n",
    "    tree = ET.parse(os.path.join(report_path, report))\n",
    "    root = tree.getroot()\n",
    "    pi = root.findall('parentImage')\n",
    "    if len(pi)<2:\n",
    "        pass\n",
    "        #print('not enough images! skipping {}'.format(report))\n",
    "    else:\n",
    "        img_dict['id'] = pi[0].attrib['id']\n",
    "        for img_id in pi:\n",
    "            img_dict['image_path'].append(img_id.attrib['id']+'.png')\n",
    "        img_dict['report'] = root.findall(\".//AbstractText[@Label='FINDINGS']\")[0].text\n",
    "        if img_dict['report'] == None:\n",
    "            continue\n",
    "        num_reports += 1\n",
    "        report_list.append(img_dict)\n",
    "        \n",
    "        \n",
    "print('{} seconds to process {} reports'.format(time.time()-start, len(reports)))\n",
    "print('Total reports in dataset: {}'.format(num_reports))\n",
    "print('Reports not included due to lack of images or lack of Report Text: {}'.format(len(reports)-num_reports))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1e7731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2955\n"
     ]
    }
   ],
   "source": [
    "# double check number of reports\n",
    "print(len(report_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbaea80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev size: 2364, test size: 591\n",
      "Train size: 2068, val size: 296\n"
     ]
    }
   ],
   "source": [
    "# use sklearn to split into train, validation, and test sets\n",
    "dev, test, _, _ = train_test_split(report_list, range(len(report_list)), test_size=0.2, random_state=42)\n",
    "print('Dev size: {}, test size: {}'.format(len(dev),len(test)))\n",
    "train, val, _, _ = train_test_split(dev, range(len(dev)), test_size=1/8, random_state=42)\n",
    "print('Train size: {}, val size: {}'.format(len(train),len(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a5a19be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to dictionary with train, val, and test partitions\n",
    "annotations = {'train': train, 'val': val, 'test': test}\n",
    "\n",
    "# convert dictionary to json object\n",
    "annotations_json = json.dumps(annotations)\n",
    "\n",
    "# write to json file\n",
    "with open(\"annotation.json\", \"w\") as f:\n",
    "    f.write(annotations_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4f9dab55187a644dc9216a574b8bf5a5414f59e9d3120c4b3155988d88cf5e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
